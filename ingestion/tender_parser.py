"""
ingestion/tender_parser.py
åŠŸèƒ½ï¼šé›†æˆ è¯­ä¹‰åˆ‡åˆ† + è¡¨æ ¼è¯­ä¹‰åŒ– + æ·±åº¦XMLæ‰«æ + OCRå›¾ç‰‡è¯†åˆ« (RapidOCR)
"""
import os
import uuid
import re
import pdfplumber
import numpy as np
from typing import List, Dict
from dataclasses import dataclass
from docx import Document
from PIL import Image

# --- 1. OCR æ¨¡å—å¼•å…¥ (æ–°å¢) ---
try:
    from rapidocr_onnxruntime import RapidOCR
    # åˆå§‹åŒ– OCR å¼•æ“ (åªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œè‡ªåŠ¨ä¸‹è½½æ¨¡å‹)
    # det: æ–‡æœ¬æ£€æµ‹, rec: æ–‡æœ¬è¯†åˆ«
    ocr_engine = RapidOCR()
    HAS_OCR = True
    print("ğŸ‘€ [Parser] RapidOCR å¼•æ“åŠ è½½æˆåŠŸ")
except ImportError:
    HAS_OCR = False
    print("âš ï¸ æœªå®‰è£… rapidocr_onnxruntimeï¼Œæ— æ³•è¯†åˆ«å›¾ç‰‡å†…å®¹")

# --- 2. LangChain ç»„ä»¶å¯¼å…¥ ---
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_experimental.text_splitter import SemanticChunker

try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    try:
        from langchain.text_splitter import RecursiveCharacterTextSplitter
    except ImportError:
        print("âŒ ä¸¥é‡é”™è¯¯: ç¼ºå°‘ langchain-text-splitters åº“")
        class RecursiveCharacterTextSplitter:
            def __init__(self, **kwargs): pass
            def split_text(self, text): return [text]

# --- 3. è‡ªå®šä¹‰å¤„ç†å™¨å¯¼å…¥ ---
try:
    from ingestion.processors import TableSummarizer, TableProcessor
except ImportError:
    TableSummarizer = None
    class TableProcessor:
        @staticmethod
        def table_to_markdown(data): return str(data)

@dataclass
class IndexableChunk:
    content: str
    metadata: Dict
    chunk_id: str
    parent_id: str = None
    is_parent: bool = False

class TenderDocParser:
    def __init__(self, project_info: Dict[str, str], use_advanced_mode: bool = True):
        self.project_info = project_info or {}
        self.use_advanced_mode = use_advanced_mode
        self.table_summarizer = TableSummarizer() if (TableSummarizer and use_advanced_mode) else None

        if self.use_advanced_mode:
            print("â³ [Parser] åˆå§‹åŒ–è¯­ä¹‰åˆ‡åˆ†æ¨¡å‹ (BGE)...")
            self.embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh-v1.5")
            self.splitter = SemanticChunker(self.embeddings, breakpoint_threshold_type="percentile")
        else:
            print("ğŸš€ [Parser] ä½¿ç”¨å¿«é€Ÿé€’å½’åˆ‡åˆ†æ¨¡å¼...")
            self.splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

    def parse_file(self, file_path: str) -> List[IndexableChunk]:
        ext = os.path.splitext(file_path)[1].lower()
        if ext == ".pdf":
            return self._parse_pdf(file_path)
        elif ext in [".docx", ".doc"]:
            return self._parse_docx(file_path)
        else:
            print(f"âš ï¸ è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}")
            return []

    # --- OCR è¾…åŠ©å‡½æ•° (æ–°å¢) ---
    def _run_ocr_on_image(self, img_obj) -> str:
        """è¾“å…¥ PIL Imageï¼Œè¿”å›è¯†åˆ«æ–‡å­—"""
        if not HAS_OCR: return ""
        try:
            # RapidOCR éœ€è¦ numpy array æ ¼å¼
            img_np = np.array(img_obj)
            # result ç»“æ„: List[ [box_coord, text, confidence] ]
            result, _ = ocr_engine(img_np)
            if result:
                # æå–è¯†åˆ«åˆ°çš„æ–‡æœ¬ï¼Œè¿‡æ»¤ä½ç½®ä¿¡åº¦ (<0.6)
                txts = [line[1] for line in result if float(line[2]) > 0.6]
                return "\n".join(txts)
        except Exception as e:
            print(f"âš ï¸ OCR è¯†åˆ«å‡ºé”™: {e}")
        return ""

    def _deep_scan_docx(self, doc) -> str:
        """
        Word æ·±åº¦ XML æ‰«æ (é’ˆå¯¹æ–‡æœ¬æ¡†)
        """
        try:
            xml = doc._element.xml
            text_nodes = re.findall(r'<w:t[^>]*>(.*?)</w:t>', xml)
            return "\n".join(text_nodes)
        except Exception as e:
            print(f"âŒ æ·±åº¦æ‰«æå¤±è´¥: {e}")
            return ""

    def _parse_docx(self, file_path: str) -> List[IndexableChunk]:
        print(f"ğŸ“ [Parser] å¤„ç† Word: {os.path.basename(file_path)}")
        all_chunks = []
        full_text_buffer = ""

        try:
            doc = Document(file_path)

            # 1. å°è¯•æå–è¡¨æ ¼
            for table in doc.tables:
                data = [[cell.text.strip() for cell in row.cells] for row in table.rows]
                content = ""
                if self.use_advanced_mode and self.table_summarizer:
                    content = self.table_summarizer.summarize_table(data)
                else:
                    content = TableProcessor.table_to_markdown(data)

                if content and len(content) > 10:
                    all_chunks.append(IndexableChunk(
                        content=content,
                        metadata={**self.project_info, "source_file": os.path.basename(file_path), "page": 1, "type": "table"},
                        chunk_id=str(uuid.uuid4())
                    ))

            # 2. å°è¯•æ ‡å‡†æå–æ–‡æœ¬ (æ®µè½)
            for para in doc.paragraphs:
                if para.text.strip():
                    full_text_buffer += para.text + "\n"

            # 3. æ·±åº¦ XML æ‰«æ (æ–‡æœ¬æ¡†)
            if not full_text_buffer.strip():
                print("âš ï¸ [Parser] æ ‡å‡†è§£ææœªå‘ç°æ–‡æœ¬ï¼Œå¯åŠ¨æ·±åº¦ XML æ‰«æ(æ–‡æœ¬æ¡†)...")
                full_text_buffer = self._deep_scan_docx(doc)
                if full_text_buffer:
                    print(f"âœ… [Parser] æ·±åº¦æ‰«ææˆåŠŸæå– {len(full_text_buffer)} å­—ç¬¦")

            # 4. çº¯å›¾ç‰‡ Word æç¤º
            if not full_text_buffer.strip() and not all_chunks:
                print("âš ï¸ [Parser] Word æ–‡ä»¶ä¼¼ä¹æ˜¯çº¯å›¾ç‰‡ï¼ŒWord OCR æå–æå…¶ä¸ç¨³å®šï¼Œå»ºè®®ç”¨æˆ·è½¬ PDF ä¸Šä¼ ã€‚")
                return []

            # 5. åˆ‡åˆ†æ–‡æœ¬
            if full_text_buffer:
                text_chunks = self._split_text(full_text_buffer)
                for txt in text_chunks:
                    if len(txt.strip()) > 5:
                        all_chunks.append(IndexableChunk(
                            content=txt,
                            metadata={**self.project_info, "source_file": os.path.basename(file_path), "page": 1, "type": "text"},
                            chunk_id=str(uuid.uuid4())
                        ))

        except Exception as e:
            print(f"âŒ Word è§£æå¤±è´¥: {e}")

        return all_chunks

    def _parse_pdf(self, file_path: str) -> List[IndexableChunk]:
        print(f"ğŸ“„ [Parser] å¤„ç† PDF: {os.path.basename(file_path)}")
        all_chunks = []
        full_text_buffer = ""

        with pdfplumber.open(file_path) as pdf:
            total_pages = len(pdf.pages)
            for page_num, page in enumerate(pdf.pages, start=1):
                page_text_buffer = ""

                # A. æå–è¡¨æ ¼
                tables = page.extract_tables()
                if tables:
                    for table in tables:
                        content = ""
                        if self.use_advanced_mode and self.table_summarizer:
                            content = self.table_summarizer.summarize_table(table)
                        else:
                            content = TableProcessor.table_to_markdown(table)
                        if content:
                            all_chunks.append(IndexableChunk(
                                content=content,
                                metadata={**self.project_info, "source_file": os.path.basename(file_path), "page": page_num, "type": "table"},
                                chunk_id=str(uuid.uuid4())
                            ))

                # B. æå–åŸç”Ÿæ–‡æœ¬
                text = page.extract_text()
                if text:
                    page_text_buffer += text + "\n"

                # C. [æ–°å¢] æ··åˆè§†è§‰è§£æ (å›¾ç‰‡ OCR)
                # æ¡ä»¶ï¼šå·²å®‰è£…OCRåº“ + å¼€å¯å¢å¼ºæ¨¡å¼ + é¡µé¢åŒ…å«å›¾ç‰‡å¯¹è±¡
                if HAS_OCR and self.use_advanced_mode and page.images:
                    try:
                        # å°†æ•´ä¸ªé¡µé¢æ¸²æŸ“ä¸ºå›¾ç‰‡ (resolution=200 å…¼é¡¾é€Ÿåº¦ä¸è¯†åˆ«ç‡)
                        # è¿™æ ·åšæ¯”æå–å•ä¸ªå›¾ç‰‡æ›´ç¨³å¥ï¼Œå› ä¸ºèƒ½ä¿ç•™å›¾ç‰‡æ’ç‰ˆä½ç½®çš„ä¸Šä¸‹æ–‡
                        pil_image = page.to_image(resolution=200).original
                        ocr_text = self._run_ocr_on_image(pil_image)

                        if ocr_text:
                            # ç®€å•å»é‡ï¼šå¦‚æœ OCR è¯†åˆ«å‡ºçš„å†…å®¹å¾ˆé•¿ï¼Œä¸”ä¸åœ¨åŸç”Ÿæ–‡æœ¬é‡Œï¼Œåˆ™è¿½åŠ 
                            # æˆ–è€…ç›´æ¥è¿½åŠ ï¼Œä¾é  Semantic Splitter å»å¤„ç†è¯­ä¹‰
                            if len(ocr_text) > 20: # è¿‡æ»¤å¤ªçŸ­çš„å™ªç‚¹
                                # æ ‡è®°è¿™éƒ¨åˆ†å†…å®¹æ¥æºäºå›¾ç‰‡/æ‰«æä»¶
                                page_text_buffer += f"\n\nã€ç¬¬{page_num}é¡µå›¾ç‰‡/æ‰«æä»¶å†…å®¹ã€‘:\n{ocr_text}\n"
                                print(f"   ğŸ‘ï¸ [OCR] ç¬¬ {page_num} é¡µæå–åˆ°å›¾ç‰‡æ–‡å­— ({len(ocr_text)} å­—ç¬¦)")
                    except Exception as e:
                        print(f"   âš ï¸ ç¬¬ {page_num} é¡µ OCR å¤„ç†å¤±è´¥: {e}")

                full_text_buffer += page_text_buffer

        if full_text_buffer:
            text_chunks = self._split_text(full_text_buffer)
            # ç®€å•é¡µç ä¼°ç®—
            total_chars = len(full_text_buffer)
            chars_per_page = total_chars / total_pages if total_pages > 0 else 1000
            current_char_idx = 0
            for txt in text_chunks:
                chunk_len = len(txt)
                mid_point = current_char_idx + (chunk_len / 2)
                est_page = min(int(mid_point / chars_per_page) + 1, total_pages)
                current_char_idx += chunk_len
                all_chunks.append(IndexableChunk(
                    content=txt,
                    metadata={**self.project_info, "source_file": os.path.basename(file_path), "page": est_page, "type": "text"},
                    chunk_id=str(uuid.uuid4())
                ))
        return all_chunks

    def _split_text(self, text: str) -> List[str]:
        if self.use_advanced_mode:
            try:
                docs = self.splitter.create_documents([text])
                return [d.page_content for d in docs]
            except Exception as e:
                print(f"âš ï¸ è¯­ä¹‰åˆ‡åˆ†å‡ºé”™ ({e})ï¼Œé™çº§ä¸ºé€’å½’åˆ‡åˆ†")
                from langchain.text_splitter import RecursiveCharacterTextSplitter
                fallback = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
                return fallback.split_text(text)
        else:
            return self.splitter.split_text(text)
