"""
etl/metadata_extractor.py
åŠŸèƒ½ï¼šåˆ©ç”¨ LLM (DeepSeek) æ™ºèƒ½åˆ†ææ–‡æ¡£å‰å‡ é¡µï¼Œæå–ç»“æ„åŒ–å…ƒæ•°æ®ã€‚
"""

import json
import re
import os
from typing import Dict
from openai import Client # ç»Ÿä¸€ä½¿ç”¨ Client

class IntelligentMetadataExtractor:
    def __init__(self, api_key=None):
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        # åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯
        self.client = Client(
            api_key=self.api_key,
            base_url="https://api.deepseek.com"
        )

    def _read_cover_pages(self, file_path: str, max_pages=3) -> str:
        """è¯»å– PDF/Word çš„å‰ N é¡µæ–‡æœ¬ç”¨äºåˆ†æ"""
        text_buffer = []
        try:
            ext = os.path.splitext(file_path)[1].lower()
            if ext == ".pdf":
                import pdfplumber
                with pdfplumber.open(file_path) as pdf:
                    # åªè¯»å‰å‡ é¡µï¼Œé˜²æ­¢ token æº¢å‡º
                    for i, page in enumerate(pdf.pages[:max_pages]):
                        text_buffer.append(page.extract_text() or "")
            elif ext in [".docx", ".doc"]:
                from docx import Document
                doc = Document(file_path)
                # Word è¯»å–å‰ 20 æ®µä½œä¸ºå°é¢å†…å®¹
                for para in doc.paragraphs[:20]:
                    text_buffer.append(para.text)
        except ImportError:
            print("âš ï¸ ç¼ºå°‘ pdfplumber æˆ– python-docx åº“ï¼Œæ— æ³•è¯»å–å°é¢ã€‚")
        except Exception as e:
            print(f"âš ï¸ è¯»å–å°é¢å¤±è´¥: {e}")

        return "\n".join(text_buffer)

    def extract(self, file_path: str) -> Dict[str, str]:
        print(f"ğŸ§  [AIæå–] æ­£åœ¨åˆ†ææ–‡æ¡£å…ƒæ•°æ®: {os.path.basename(file_path)}...")

        context_text = self._read_cover_pages(file_path)

        # å¦‚æœè¯»ä¸åˆ°å†…å®¹ï¼Œæˆ–è€…æ²¡æœ‰é…ç½® API Keyï¼Œç›´æ¥å›é€€
        if not context_text or not self.api_key:
            return self._fallback_extraction(file_path)

        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªæ°´åˆ©å·¥ç¨‹æ‹›æŠ•æ ‡ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ ‡ä¹¦çš„å‰å‡ é¡µå†…å®¹ä¸­ï¼Œæå–å…³é”®å…ƒæ•°æ®ã€‚
        å¦‚æœæ‰¾ä¸åˆ°æŸé¡¹ä¿¡æ¯ï¼Œè¯·ç•™ç©ºã€‚

        ã€å¾…åˆ†æå†…å®¹ã€‘
        {context_text[:2000]} ... (æˆªæ–­)

        ã€è¦æ±‚ã€‘
        è¯·ä»…è¿”å›ä¸€ä¸ªåˆæ³•çš„ JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å« markdown æ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        1. "project_name": é¡¹ç›®å…¨ç§°
        2. "province": çœä»½ (å¦‚: å››å·çœ)
        3. "year": æ‹›æ ‡å¹´ä»½ (æ ¼å¼: YYYY)
        4. "type": å·¥ç¨‹ç±»å‹ (ä»ä»¥ä¸‹åˆ—è¡¨ä¸­é€‰æ‹©æœ€åŒ¹é…çš„ä¸€ä¸ª: æ°´åº“, å ¤é˜², æ²³é“, æ³µç«™, æ°´ç”µç«™, çŒåŒº, é¥®æ°´å®‰å…¨)

        JSON:
        """

        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                response_format={"type": "json_object"}
            )

            content = response.choices[0].message.content
            # æ¸…ç†å¯èƒ½çš„ markdown æ ‡è®°
            content = content.replace("```json", "").replace("```", "")
            meta = json.loads(content)

            # è¡¥å……æºæ–‡ä»¶å
            meta["source_file"] = os.path.basename(file_path)
            meta["source_method"] = "ai_extraction"

            # ç¡®ä¿ type å­—æ®µå­˜åœ¨ï¼Œç”¨äºåç»­è¿‡æ»¤
            if "type" not in meta: meta["type"] = "å…¶ä»–"

            return meta

        except Exception as e:
            print(f"âŒ AI æå–å¤±è´¥: {e}ï¼Œåˆ‡æ¢åˆ°æ–‡ä»¶ååŒ¹é…æ¨¡å¼...")
            return self._fallback_extraction(file_path)

    def _fallback_extraction(self, file_path: str) -> Dict[str, str]:
        """å›é€€é€»è¾‘ï¼šåŸºäºæ–‡ä»¶åè¿›è¡Œæ­£åˆ™å’Œå…³é”®è¯åŒ¹é…"""
        filename = os.path.basename(file_path)
        filename_no_ext = os.path.splitext(filename)[0]

        meta = {
            "project_name": filename_no_ext,
            "source_file": filename,
            "source_method": "filename_rule_match",
            "type": "å…¶ä»–" # é»˜è®¤å€¼
        }

        # 1. æå–å¹´ä»½
        year_match = re.search(r'(202\d)', filename)
        if year_match: meta["year"] = year_match.group(1)

        # 2. æå–ç±»å‹
        type_map = {
            "æ°´åº“": "æ°´åº“", "å¤§å": "æ°´åº“", "é™¤é™©": "æ°´åº“",
            "å ¤é˜²": "å ¤é˜²", "å ¤": "å ¤é˜²", "é˜²æ´ª": "å ¤é˜²",
            "æ²³é“": "æ²³é“", "æ¸…æ·¤": "æ²³é“",
            "æ³µç«™": "æ³µç«™", "ç”µç«™": "æ°´ç”µç«™",
            "çŒåŒº": "çŒåŒº", "é¥®æ°´": "é¥®æ°´å®‰å…¨"
        }
        for keyword, p_type in type_map.items():
            if keyword in filename:
                meta["type"] = p_type
                break

        # 3. æå–çœä»½
        provinces = ["å››å·", "äº‘å—", "è´µå·", "å¹¿ä¸œ", "å¹¿è¥¿", "æ¹–å—", "æ¹–åŒ—", "æ±Ÿè¥¿", "é‡åº†", "æ²³å—", "æ²³åŒ—", "æ–°ç–†", "è¥¿è—"]
        for prov in provinces:
            if prov in filename:
                meta["province"] = prov
                break

        return meta
