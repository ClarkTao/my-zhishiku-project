"""
etl/vector_store.py
åŠŸèƒ½ï¼šç®¡ç†å‘é‡æ•°æ®åº“ (ChromaDB)
ä¿®æ­£ï¼šä½¿ç”¨â€˜é€‚é…å™¨æ¨¡å¼â€™å®ç° LangChain å…¼å®¹ï¼Œè§£å†³å†…å­˜åŒå€å ç”¨å’Œå‘é‡ä¸ä¸€è‡´é—®é¢˜ã€‚
"""
import os
import chromadb
from sentence_transformers import SentenceTransformer
from modelscope import snapshot_download
from typing import List, Any

# --- å¼•å…¥ LangChain åŸºç¡€ç±» ---
try:
    from langchain_community.vectorstores import Chroma as LangChainChroma
    from langchain_core.embeddings import Embeddings # å¼•å…¥åŸºç±»
except ImportError:
    pass

CHROMA_PATH = "chroma_db"

# âœ… é›¶å¼€é”€é€‚é…å™¨
# è¿™ä¸ªç±»åªæ˜¯ä¸ªâ€œä¼ è¯ç­’â€ï¼Œå®ƒä¸å å†…å­˜ï¼Œç›´æ¥è°ƒç”¨åŸæœ‰çš„ embedding_model
class LightweightEmbeddings(Embeddings):
    def __init__(self, transformer_model):
        self.model = transformer_model

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        # ç›´æ¥å¤ç”¨ ETL çš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œä¿è¯å‘é‡ 100% ä¸€è‡´
        return self.model.encode(texts, normalize_embeddings=True).tolist()

    def embed_query(self, text: str) -> List[float]:
        # query ä¹Ÿæ˜¯åŒç†
        return self.model.encode([text], normalize_embeddings=True)[0].tolist()


class VectorStoreManager:
    def __init__(self, collection_name="tender_docs"):
        print("â³ [ETL] æ­£åœ¨åˆå§‹åŒ– Embedding æ¨¡å‹ (BGE-Small)...")
        try:
            model_dir = snapshot_download('Xorbits/bge-small-zh-v1.5')
        except:
            model_dir = "BAAI/bge-small-zh-v1.5"

        # 1. åŸç”Ÿ Embedding (ETL æ ¸å¿ƒ)
        self.embedding_model = SentenceTransformer(model_dir)

        print(f"â³ [ETL] è¿æ¥å‘é‡æ•°æ®åº“: {CHROMA_PATH}")
        # 2. åŸç”Ÿ Client (ETL æ ¸å¿ƒ)
        self.client = chromadb.PersistentClient(path=CHROMA_PATH)
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )

        # --- âœ… ä¿®æ”¹ç‚¹ï¼šæ„å»º LangChain å…¼å®¹å±‚ (æ— å‰¯ä½œç”¨ç‰ˆ) ---
        print("ğŸ”Œ [Bridge] æ­£åœ¨æ„å»º LangChain å…¼å®¹å±‚...")
        try:
            # A. ä½¿ç”¨é€‚é…å™¨ï¼Œè€Œä¸æ˜¯é‡æ–°åŠ è½½æ¨¡å‹ï¼(è§£å†³å†…å­˜é—®é¢˜ + ä¸€è‡´æ€§é—®é¢˜)
            # æˆ‘ä»¬æŠŠ self.embedding_model ä¼ è¿›å»
            bridge_embeddings = LightweightEmbeddings(self.embedding_model)

            # B. åˆå§‹åŒ– LangChain Chroma
            # è¿™é‡Œçš„ client=self.client è§£å†³äº† SQLite é”å†²çªé—®é¢˜
            self.vector_store = LangChainChroma(
                client=self.client,
                collection_name=collection_name,
                embedding_function=bridge_embeddings
            )
            print("âœ… [Bridge] LangChain VectorStore å°±ç»ª (å…±äº«å†…å­˜ä¸è¿æ¥)")

        except Exception as e:
            print(f"âš ï¸ LangChain å…¼å®¹å±‚åˆå§‹åŒ–å¤±è´¥: {e}")
            self.vector_store = None


    def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        # åŸæœ‰ ETL é€»è¾‘ä¿æŒä¸å˜
        return self.embedding_model.encode(texts, normalize_embeddings=True).tolist()

    def add_chunks(self, chunks: List[Any]):
        """
        åŸç”Ÿå…¥åº“é€»è¾‘ (ä¿æŒä¸å˜)
        """
        if not chunks: return

        # 1. é¢„æ„å»º Parent Map
        parent_map = {c.chunk_id: c.content for c in chunks if getattr(c, 'is_parent', False)}

        ids = []
        documents = []
        metadatas = []
        texts_to_embed = []

        for chunk in chunks:
            # è¿‡æ»¤è¶…é•¿çˆ¶å—
            if getattr(chunk, 'is_parent', False) and len(chunk.content) > 800:
                continue

            meta = chunk.metadata.copy()

            # çˆ¶å­èåˆ
            parent_id = getattr(chunk, 'parent_id', None)
            if parent_id and parent_id in parent_map:
                meta["full_context"] = parent_map[parent_id]
                meta["is_child"] = "True"
            else:
                meta["full_context"] = chunk.content
                meta["is_child"] = "False"

            # æ¸…æ´— Metadata
            clean_meta = {}
            for k, v in meta.items():
                if v is None:
                    clean_meta[k] = ""
                elif isinstance(v, (list, dict)):
                    clean_meta[k] = str(v)
                else:
                    clean_meta[k] = v

            ids.append(chunk.chunk_id)
            documents.append(chunk.content)
            metadatas.append(clean_meta)
            texts_to_embed.append(chunk.content)

        if not ids:
            print("âš ï¸ æ— æœ‰æ•ˆæ•°æ®å…¥åº“")
            return

        # 2. ç”Ÿæˆå‘é‡
        print(f"âš¡ [ETL] è®¡ç®—å‘é‡ä¸­ ({len(texts_to_embed)} æ¡)...")
        embeddings = self._generate_embeddings(texts_to_embed)

        # 3. æ‰¹é‡å†™å…¥
        batch_size = 100
        for i in range(0, len(ids), batch_size):
            end = min(i + batch_size, len(ids))
            self.collection.upsert(
                ids=ids[i:end],
                documents=documents[i:end],
                metadatas=metadatas[i:end],
                embeddings=embeddings[i:end]
            )
        print(f"âœ… [ETL] æˆåŠŸå­˜å…¥ {len(ids)} æ¡æ•°æ®ã€‚")

    def search(self, query: str, top_k: int = 5, filters: dict = None):
        """
        åŸç”Ÿæ£€ç´¢å°è£…
        """
        query_vec = self._generate_embeddings([query])
        return self.collection.query(
            query_embeddings=query_vec,
            n_results=top_k,
            where=filters
        )
