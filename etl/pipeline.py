"""
etl/pipeline.py
é«˜çº§ ETL æµæ°´çº¿ (é›†æˆç‰ˆ)ã€‚
"""

import os
import uuid
import pandas as pd  # âœ… æ–°å¢ï¼šå¼•å…¥ pandas å¤„ç† Excel
from typing import Dict, List, Any

# å¼•å…¥æ‰€æœ‰ç»„ä»¶
try:
    from ingestion.tender_parser import TenderDocParser, IndexableChunk # âœ… ç¡®ä¿å¼•å…¥äº† IndexableChunk
    from etl.text_cleaner import TextCleaner
    from etl.vector_store import VectorStoreManager
    from etl.deduplication import DeduplicationService
    from etl.metadata_extractor import IntelligentMetadataExtractor
    from ingestion.metadata_manager import ProjectRegistry
    from utils.graph_manager import GraphManager
except ImportError as e:
    print(f"âš ï¸ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

class ETLPipeline:
    def __init__(self, deepseek_api_key: str):
        self.api_key = deepseek_api_key
        self.dedup = DeduplicationService()
        self.meta_extractor = IntelligentMetadataExtractor(api_key=deepseek_api_key)
        self.cleaner = TextCleaner()
        self.vector_store = VectorStoreManager()
        self.registry = ProjectRegistry()

        try:
            self.graph_manager = GraphManager()
            print("ğŸ•¸ï¸ GraphManager è¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ GraphManager åˆå§‹åŒ–å¤±è´¥ (éé˜»æ–­): {e}")
            self.graph_manager = None

    # âœ… æœ€ç»ˆå®Œç¾ç‰ˆï¼šæ—¢ä¿ç•™æ–‡æœ¬æ ¼å¼ï¼Œåˆæ”¯æŒæ™ºèƒ½ç»Ÿè®¡
    def _parse_excel(self, file_path: str, meta: Dict) -> List[IndexableChunk]:
        chunks = []
        try:
            print(f"ğŸ“Š [Parser] æ­£åœ¨è§£æ Excel (å®Œç¾ç‰ˆ): {file_path}")
            xls = pd.ExcelFile(file_path)

            for sheet_name in xls.sheet_names:
                # 1. æ ¸å¿ƒæ”¹åŠ¨ï¼šå¼ºåˆ¶æ‰€æœ‰æ•°æ®æŒ‰â€œæ–‡æœ¬â€è¯»å–ï¼Œç¡®ä¿æ‰‹æœºå·/ç¼–å·ä¸èµ°æ ·
                df = pd.read_excel(xls, sheet_name=sheet_name, dtype=str, keep_default_na=False)

                stats_desc = []
                total_rows = len(df)

                # --- ğŸ§  æ™ºèƒ½åˆ—åˆ†æ ---
                for col in df.columns:
                    # è·³è¿‡ç©ºåˆ—
                    if df[col].str.strip().eq("").all():
                        continue

                    series = df[col]

                    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼è¿›è¡Œåˆ†æ (errors='coerce' ä¼šæŠŠéæ•°å­—å˜ NaN)
                    numeric_series = pd.to_numeric(series, errors='coerce')
                    valid_count = numeric_series.count()

                    # å°è¯•è½¬æ¢ä¸ºæ—¥æœŸ
                    try:
                        date_series = pd.to_datetime(series, errors='coerce')
                        valid_date_count = date_series.count()
                    except:
                        valid_date_count = 0

                    # ğŸ” åˆ¤å®š A: è¿™ä¸€åˆ—ä¸»è¦æ˜¯æ•°å­— (æœ‰æ•ˆæ•°å­—å æ¯” > 80% ä¸”ä¸æ˜¯çº¯ ID)
                    if valid_count > total_rows * 0.8 and series.nunique() > 10:
                        _min = numeric_series.min()
                        _max = numeric_series.max()
                        _mean = numeric_series.mean()
                        _sum = numeric_series.sum()
                        stats_desc.append(
                            f"ã€æ•°å€¼ç»Ÿè®¡ã€‘'{col}': èŒƒå›´[{_min} ~ {_max}], æ€»å’Œ={_sum:,.2f}, å‡å€¼={_mean:,.2f}"
                        )

                    # ğŸ” åˆ¤å®š B: è¿™ä¸€åˆ—ä¸»è¦æ˜¯æ—¥æœŸ
                    elif valid_date_count > total_rows * 0.8:
                        _start = date_series.min().strftime('%Y-%m-%d')
                        _end = date_series.max().strftime('%Y-%m-%d')
                        stats_desc.append(f"ã€æ—¶é—´è·¨åº¦ã€‘'{col}': ä» {_start} åˆ° {_end}")

                    # ğŸ” åˆ¤å®š C: æ–‡æœ¬/åˆ†ç±» (æ’é™¤å…¨æ˜¯æ•°å­—çš„æƒ…å†µï¼Œé¿å…æŠŠé‡‘é¢å½“åˆ†ç±»ç»Ÿè®¡)
                    elif valid_count < total_rows * 0.5:
                        str_series = series.astype(str).str.strip()
                        # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²
                        str_series = str_series[str_series != ""]
                        unique_count = str_series.nunique()

                        if unique_count <= 30:
                            counts = str_series.value_counts()
                            stats_str = ", ".join([f"{k}:{v}ä¸ª" for k, v in counts.items()])
                            stats_desc.append(f"ã€åˆ†å¸ƒç»Ÿè®¡ã€‘'{col}': {stats_str}")
                        elif unique_count < total_rows * 0.8:
                            top10 = str_series.value_counts().head(10)
                            stats_str = ", ".join([f"{k}:{v}ä¸ª" for k, v in top10.items()])
                            stats_desc.append(f"ã€é«˜é¢‘ç»Ÿè®¡ã€‘'{col}' (å‰10å): {stats_str}, ä»¥åŠå…¶ä»–...")

                # ç”Ÿæˆæ‘˜è¦åˆ‡ç‰‡
                if stats_desc:
                    summary_text = (
                            f"ã€è¡¨æ ¼å…¨æ™¯ç»Ÿè®¡-{sheet_name}ã€‘\n"
                            f"æ•°æ®æ€»è¡Œæ•°: {total_rows} è¡Œ\n"
                            "ä»¥ä¸‹æ˜¯å…³é”®å­—æ®µçš„è‡ªåŠ¨åˆ†æç»“æœï¼š\n" +
                            "\n".join(stats_desc)
                    )
                    chunks.append(IndexableChunk(
                        chunk_id=str(uuid.uuid4()),
                        content=summary_text,
                        metadata={
                            "source_file": meta.get("source_file", ""),
                            "project_name": meta.get("project_name", ""),
                            "category": "ç»Ÿè®¡æ‘˜è¦",
                            "page": sheet_name,
                            "type": "summary"
                        }
                    ))

                # --- è¡Œæ•°æ®è½¬æ¢ (ç°åœ¨ df æœ¬èº«å°±æ˜¯ strï¼Œç›´æ¥ç”¨å³å¯ï¼Œæ— éœ€å†è½¬æ¢) ---
                for index, row in df.iterrows():
                    row_items = []
                    for col in df.columns:
                        val = str(row[col]).strip()
                        if val and val.lower() not in ['nan', 'none', '', 'null']:
                            # ç®€å•æ¸…æ´—
                            if val.endswith(" 00:00:00"): val = val.replace(" 00:00:00", "")
                            row_items.append(f"{col}: {val}")

                    if row_items:
                        content_str = f"ã€è¡¨æ ¼æ•°æ®-{sheet_name}ã€‘ " + "; ".join(row_items)
                        chunks.append(IndexableChunk(
                            chunk_id=str(uuid.uuid4()),
                            content=content_str,
                            metadata={
                                "source_file": meta.get("source_file", ""),
                                "project_name": meta.get("project_name", ""),
                                "category": meta.get("category", "è¡¨æ ¼æ•°æ®"),
                                "page": sheet_name,
                                "type": "table"
                            }
                        ))
            return chunks
        except Exception as e:
            print(f"âŒ Excel è§£æå¤±è´¥: {e}")
            return []

    def process_file(self, file_path: str, use_advanced_mode: bool = True, force_update: bool = False,
                     original_filename: str = None,
                     user_project: str = None,
                     user_tag: str = None
                     ) -> Dict[str, Any]:

        current_file_name = os.path.basename(file_path)
        display_name = original_filename if original_filename else current_file_name

        result = {"file": display_name, "status": "pending", "chunks": 0, "msg": ""}
        print(f"\nğŸš€ [Pipeline] å¯åŠ¨: {display_name} (å¢å¼ºæ¨¡å¼: {use_advanced_mode})")

        # --- Step 1: æŸ¥é‡ ---
        if not force_update and self.dedup.is_processed(file_path):
            result["status"] = "skipped"
            result["msg"] = "æ–‡ä»¶å†…å®¹æŒ‡çº¹å·²å­˜åœ¨"
            print(f"â­ï¸ {display_name} å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            return result

        # --- Step 2: AI æå–å…ƒæ•°æ® ---
        meta = {}
        if use_advanced_mode:
            try:
                meta = self.meta_extractor.extract(file_path)
                print(f"ğŸ§  [Metadata] AIæå–ç»“æœ: {meta}")
            except Exception as e:
                print(f"âš ï¸ AI å…ƒæ•°æ®æå–å¤±è´¥ ({e})ï¼Œå›é€€åˆ°é»˜è®¤è®¾ç½®")
                meta = {}

        # å¼ºåˆ¶è¦†ç›–å…ƒæ•°æ®
        meta["source_file"] = display_name
        if user_project:
            meta["project_name"] = user_project
        elif "project_name" not in meta or not meta["project_name"]:
            meta["project_name"] = os.path.splitext(display_name)[0]

        if user_tag:
            meta["category"] = user_tag

        self.registry.register_project(meta.get("project_name", "Unknown"), meta)

        # --- Step 3: è§£æ (Parsing) ---
        chunks = []
        try:
            file_ext = os.path.splitext(file_path)[1].lower()

            # âœ… [ä¿®æ”¹]ï¼šåˆ†æ”¯åˆ¤æ–­ï¼Œæ”¯æŒ Excel
            if file_ext in ['.xlsx', '.xls']:
                chunks = self._parse_excel(file_path, meta)
            else:
                # åŸæœ‰çš„ Word/PDF è§£æé€»è¾‘
                parser = TenderDocParser(project_info=meta, use_advanced_mode=use_advanced_mode)
                chunks = parser.parse_file(file_path)

        except Exception as e:
            result["status"] = "error"
            result["msg"] = f"è§£æå¤±è´¥: {str(e)}"
            return result

        if not chunks:
            result["status"] = "warning"
            result["msg"] = "æœªæå–åˆ°æœ‰æ•ˆå†…å®¹"
            return result

        # --- Step 4: æ¸…æ´— & ID ç”Ÿæˆ ---
        cleaned_chunks = []
        for i, chunk in enumerate(chunks):
            # åªæœ‰é Excel çš„æ‰éœ€è¦æ·±åº¦æ¸…æ´— (Excel å·²ç»æ˜¯ç»“æ„åŒ–æ–‡æœ¬äº†)
            if chunk.metadata.get("type") != "table":
                chunk.content = self.cleaner.clean(chunk.content)

            if chunk.metadata.get("type") == "table" or len(chunk.content) > 5:
                if original_filename:
                    chunk.metadata["source_file"] = original_filename
                chunk.metadata["project_name"] = meta.get("project_name", "Unknown")
                if "category" in meta:
                    chunk.metadata["category"] = meta["category"]

                unique_str = f"{display_name}_{i}"
                chunk_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, unique_str))
                chunk.metadata["chunk_id"] = chunk_id

                cleaned_chunks.append(chunk)

        # --- Step 5: å…¥åº“ ---
        if cleaned_chunks:
            self.vector_store.add_chunks(cleaned_chunks)

            if self.graph_manager:
                try:
                    graph_data = [{
                        "content": c.content,
                        "page": c.metadata.get("page", 0),
                        "chunk_id": c.metadata["chunk_id"]
                    } for c in cleaned_chunks]

                    self.graph_manager.create_document_structure(
                        filename=display_name,
                        project=meta.get("project_name", "Unknown"),
                        chunks=graph_data
                    )
                    print(f"ğŸ•¸ï¸ [Graph] å·²æ„å»º {len(graph_data)} ä¸ªèŠ‚ç‚¹çš„å›¾è°±é“¾")
                except Exception as e:
                    print(f"âš ï¸ å›¾è°±å†™å…¥å¼‚å¸¸ (ä¸å½±å“å‘é‡åº“): {e}")

            self.dedup.mark_as_processed(file_path)
            result["status"] = "success"
            result["chunks"] = len(cleaned_chunks)
            result["msg"] = "å…¥åº“æˆåŠŸ"
            print(f"âœ… [Success] {display_name} å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(cleaned_chunks)} ä¸ªåˆ‡ç‰‡ã€‚")
        else:
            result["status"] = "warning"
            result["msg"] = "æ¸…æ´—åæ— æœ‰æ•ˆæ•°æ®"

        return result
