"""
generation/rag_service.py
æ——èˆ°ä¿®å¤ç‰ˆ v3.6 (UXä¼˜åŒ–ç‰ˆ)ï¼šåŠ å…¥å®¡æ ¸æ¨¡å¼æç¤ºä¸çŠ¶æ€æ˜¾å¼æ›´æ–°
"""
import os
import sys
import io
import json
import re
import pandas as pd
import hashlib
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Generator, Union, Optional, Any

try:
    from langchain_community.chat_models import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.documents import Document
    from langchain_core.messages import HumanMessage, SystemMessage
except ImportError as e:
    raise ImportError(f"LangChain æ ¸å¿ƒä¾èµ–ç¼ºå¤±: {e}")

# ==============================================================================
# ğŸ§© æ¨¡å—ç‹¬ç«‹å¯¼å…¥åŒº (å…³é”®ä¿®å¤ï¼šé˜²æ­¢ä¸€ä¸ªç¼ºå¤±å¯¼è‡´å…¨ç›˜å¤±è´¥)
# ==============================================================================

# 1. å‘é‡åº“ç®¡ç†å™¨ (æ ¸å¿ƒ)
VectorStoreManager = None
try:
    from etl.vector_store import VectorStoreManager
except ImportError as e:
    print(f"âš ï¸ [åˆå§‹åŒ–è­¦å‘Š] VectorStoreManager å¯¼å…¥å¤±è´¥: {e}")

# 2. Rerank é‡æ’åº (å¯é€‰)
RerankService = None
try:
    from generation.reranker import RerankService
except ImportError:
    pass  # é™é»˜å¤±è´¥ï¼Œè§†ä¸ºæœªå¯ç”¨

# 3. GraphRAG å›¾è°± (å¯é€‰)
GraphManager = None
try:
    from utils.graph_manager import GraphManager
except ImportError:
    pass

# 4. BM25 æ£€ç´¢ (å¯é€‰)
BM25Persistence = None
try:
    from utils.bm25_manager import BM25Persistence
except ImportError:
    pass

# 5. æ–‡ä¹¦ç”Ÿæˆå¼•æ“ (æ ¸å¿ƒ - å¿…é¡»ç¡®ä¿ç‹¬ç«‹å¯¼å…¥)
TenderWriterEngine = None
try:
    from utils.tender_engine import TenderWriterEngine
except ImportError as e:
    print(f"âš ï¸ [åˆå§‹åŒ–è­¦å‘Š] TenderWriterEngine å¯¼å…¥å¤±è´¥ (è¯·æ£€æŸ¥ utils/tender_engine.py æ˜¯å¦å­˜åœ¨): {e}")


# ==============================================================================
# ğŸš€ ä¸»æœåŠ¡ç±»
# ==============================================================================
class DeepSeekRAGService:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.model_name = "deepseek-chat"
        self.llm = ChatOpenAI(
            model_name=self.model_name,
            openai_api_key=api_key,
            openai_api_base="https://api.deepseek.com",
            temperature=0.3,
            streaming=True
        )

        # --- åˆå§‹åŒ–å„ä¸ªç»„ä»¶ (å¸¦ç‹¬ç«‹é”™è¯¯æ•è·) ---

        # 1. VectorStore
        self.vector_store = None
        if VectorStoreManager:
            try:
                self.vs_manager = VectorStoreManager()
                self.vector_store = self.vs_manager.vector_store
            except Exception as e:
                print(f"âŒ å‘é‡åº“åˆå§‹åŒ–å¼‚å¸¸: {e}")

        self.data_repo_dir = "data_repository"

        # 2. Reranker
        self.reranker = None
        if RerankService:
            try: self.reranker = RerankService()
            except: pass

        # 3. GraphRAG
        self.graph_manager = None
        if GraphManager:
            try: self.graph_manager = GraphManager()
            except: pass

        # 4. BM25
        self.bm25_manager = None
        if BM25Persistence:
            try: self.bm25_manager = BM25Persistence()
            except: pass

        # 5. Writer Engine (å…³é”®ä¿®å¤)
        self.writer_engine = None
        if TenderWriterEngine:
            try:
                self.writer_engine = TenderWriterEngine(api_key=api_key)
                print("âœ… æ–‡ä¹¦ç”Ÿæˆå¼•æ“ (TenderWriterEngine) åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ æ–‡ä¹¦ç”Ÿæˆå¼•æ“åˆå§‹åŒ–æŠ¥é”™: {e}")
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ° TenderWriterEngine ç±»å®šä¹‰ï¼Œå†™ä½œåŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")

        # Prompt åŒ…å«å¯¹è¯å†å²
        self.prompt = ChatPromptTemplate.from_template("""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ°´åˆ©å·¥ç¨‹æ ‡ä¹¦åˆ†æä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆContextï¼‰å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        
        ã€å¯¹è¯å†å²ã€‘
        {chat_history}

        ã€æ£€ç´¢ä¸Šä¸‹æ–‡ã€‘
        {context}

        ã€ç”¨æˆ·é—®é¢˜ã€‘
        {question}
        
        è¯·åšå‡ºä¸“ä¸šã€ä¾æ®å……åˆ†çš„å›ç­”ã€‚
        """)

    def rewrite_query(self, query: str) -> str:
        try:
            prompt = ChatPromptTemplate.from_template("è¯·å°†æ­¤ç”¨æˆ·é—®é¢˜é‡å†™ä¸ºæ›´é€‚åˆæœç´¢çš„æŸ¥è¯¢ï¼š{question}")
            chain = prompt | self.llm | StrOutputParser()
            return chain.invoke({"question": query}).strip()
        except:
            return query

    def _detect_excel_task(self, docs: List[Document]) -> Optional[tuple]:
        if not docs: return None
        excel_votes = {}
        for doc in docs:
            meta = doc.metadata
            src = meta.get("source_file", "")
            if src.lower().endswith((".xlsx", ".xls")) and meta.get("type") in ["table", "summary", "sheet"]:
                project = meta.get("project_name", "")
                full_path = os.path.join(self.data_repo_dir, project, src)
                key = (full_path, meta.get("page", 0))
                excel_votes[key] = excel_votes.get(key, 0) + 1
        if not excel_votes: return None
        return max(excel_votes.items(), key=lambda x: x[1])[0]

    def _run_pandas_agent(self, file_path: str, sheet_name: str, query: str) -> str:
        try:
            df = pd.read_excel(file_path, sheet_name=sheet_name, dtype=str)
        except:
            try:
                df = pd.read_excel(file_path, dtype=str)
            except Exception as e:
                return f"è¯»å– Excel å¤±è´¥: {e}"

        sys_prompt = f"ä½ æ˜¯ä¸€ä½èµ„æ·±æ•°æ®åˆ†æå¸ˆã€‚å½“å‰ dataframe å˜é‡åä¸º dfã€‚ç”¨æˆ·è¯‰æ±‚ï¼š{query}ã€‚è¯·ç›´æ¥ç¼–å†™ Python ä»£ç è§£å†³ï¼Œä¸è¦è§£é‡Šã€‚"

        original_streaming = self.llm.streaming
        self.llm.streaming = False
        try:
            response = self.llm.invoke([SystemMessage(content=sys_prompt), HumanMessage(content="è¯·å†™ä»£ç ã€‚")])
            code = response.content.replace("```python", "").replace("```", "").strip()
        except Exception as e:
            return f"ç”Ÿæˆä»£ç å¤±è´¥: {e}"
        finally:
            self.llm.streaming = original_streaming

        old_stdout = sys.stdout
        redirected_output = io.StringIO()
        sys.stdout = redirected_output
        try:
            exec_globals = {"df": df, "pd": pd}
            exec(code, exec_globals)
            sys.stdout = old_stdout
            return redirected_output.getvalue()
        except Exception as e:
            sys.stdout = old_stdout
            return f"åˆ†ææ‰§è¡Œå‡ºé”™: {e}\nä»£ç :\n{code}"

        # ==========================================
        # ğŸ“ Writer Agent (æœ€ç»ˆä¿®å¤ç‰ˆï¼šä¿®æ­£å‚è€ƒä¸ç”Ÿæˆçš„é€»è¾‘å€’ç½®)
        # ==========================================
    def _run_writer_agent_stream(self, query: str) -> Generator[Dict, None, None]:
        if not self.writer_engine:
            yield {"type": "text", "data": "âŒ æ–‡ä¹¦ç”Ÿæˆå¼•æ“æœªåˆå§‹åŒ–ï¼ˆå¯èƒ½ç¼ºå°‘ä¾èµ–æ–‡ä»¶ utils/tender_engine.pyï¼‰ã€‚"}
            return

        yield {"type": "status", "data": "ğŸš€ æ”¶åˆ°å†™ä½œæŒ‡ä»¤ï¼Œæ­£åœ¨å¯åŠ¨ã€æ–‡ä¹¦ç”Ÿæˆå·¥åŠã€‘..."}

        try:
            # 1. æå–å‚æ•° (ä¿®å¤æ ¸å¿ƒï¼šæ˜ç¡®åŒºåˆ† å‚è€ƒæ–‡ä»¶(Source) å’Œ æ–°æ ‡é¢˜(Target))
            yield {"type": "status", "data": "ğŸ” æ™ºèƒ½è§£ææ„å›¾..."}
            self.llm.streaming = False

            # âœ… [å…³é”®ä¿®æ”¹] Prompt æ˜ç¡®åŒºåˆ†â€œå‚è€ƒå¯¹è±¡â€å’Œâ€œç”Ÿæˆå¯¹è±¡â€
            extract_prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„å‚æ•°æå–åŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·æŒ‡ä»¤ï¼Œæå–ä»¥ä¸‹ä¸‰ä¸ªå…³é”®å­—æ®µï¼Œå¹¶è¿”å›çº¯ JSON æ ¼å¼ï¼š

            ã€ç”¨æˆ·æŒ‡ä»¤ã€‘
            "{query}"

            ã€æå–è§„åˆ™ã€‘
            1. "reference_filename": ç”¨æˆ·æŒ‡å®šçš„**å‚è€ƒèŒƒæ–‡**æ–‡ä»¶åï¼ˆå³å·²å­˜åœ¨çš„ã€éœ€è¦è¢«æ¨¡ä»¿çš„æ–‡ä»¶ï¼‰ã€‚
               - å…³é”®è¯ï¼šå‚è€ƒã€ä»¿ç…§ã€åŸºäºã€æ ¹æ®ã€‚
               - ä¾‹å¦‚ï¼š"å‚è€ƒã€ŠA.docxã€‹" -> æå– "A.docx"ã€‚
            2. "new_project_title": ç”¨æˆ·æƒ³è¦**æ–°å»º**çš„æ–‡æ¡£æ ‡é¢˜ã€‚
               - å…³é”®è¯ï¼šæ’°å†™ã€ç”Ÿæˆã€å†™ä¸€ä»½ã€‚
               - ä¾‹å¦‚ï¼š"å†™ä¸€ä»½ã€ŠB.docxã€‹" -> æå– "B.docx"ã€‚
            3. "new_project_info": å…³äºæ–°é¡¹ç›®çš„èƒŒæ™¯æè¿°ã€å»ºè®¾å†…å®¹ç­‰æ‰€æœ‰ä¿¡æ¯ã€‚

            ã€è¿”å›æ ¼å¼ã€‘
            {{ 
                "reference_filename": "...", 
                "new_project_title": "...", 
                "new_project_info": "..." 
            }}
            """

            resp = self.llm.invoke(extract_prompt)
            self.llm.streaming = True

            # è§£æ JSON
            content = resp.content.replace("```json", "").replace("```", "").strip()
            try:
                params = json.loads(content)
            except json.JSONDecodeError:
                # å…œåº•ï¼šå¦‚æœ JSON è§£æå¤±è´¥ï¼Œå°è¯•ç”¨æ­£åˆ™æå–
                params = {}

            # è·å–å‚æ•°
            ref_filename = params.get("reference_filename")
            new_title = params.get("new_project_title", "æœªå‘½åæ–‡æ¡£")
            project_info = params.get("new_project_info", query)

            # å¦‚æœæå–å¤±è´¥ï¼Œæˆ–è€…æå–æˆäº†åŒä¸€ä¸ªï¼Œåšç®€å•çš„é€»è¾‘ä¿®æ­£
            if ref_filename and new_title and ref_filename == new_title:
                # AI å¯èƒ½ä¼šæ··æ·†ï¼Œè¿™é‡Œç®€å•åˆ¤æ–­ï¼šå¦‚æœæ–‡ä»¶ååŒ…å« "å‚è€ƒ"ï¼Œåˆ™å¯èƒ½æ˜¯å‚è€ƒæ–‡ä»¶
                pass

            if not ref_filename:
                yield {"type": "text", "data": "âŒ æ— æ³•è¯†åˆ«å‚è€ƒæ–‡ä»¶ã€‚è¯·æ˜ç¡®è¯´æ˜â€œå‚è€ƒ xxæ–‡ä»¶â€ã€‚"}
                return

            print(f"ğŸ” [è§£æç»“æœ] å‚è€ƒ: {ref_filename} | æ–°å»º: {new_title}")

        except Exception as e:
            self.llm.streaming = True
            yield {"type": "text", "data": f"âŒ è§£ææŒ‡ä»¤å¤±è´¥: {e}"}
            return

        # =======================================================
        # 2. å®šä½æ–‡ä»¶ (ä½¿ç”¨ ref_filename å»ç¡¬ç›˜æ‰¾ï¼Œè€Œä¸æ˜¯ç”¨ new_title)
        # =======================================================
        ref_path = None
        # å½’ä¸€åŒ–å‚è€ƒæ–‡ä»¶å
        target_pure = ref_filename.lower().replace(" ", "").replace("ã€Š", "").replace("ã€‹", "").replace(".docx",
                                                                                                      "")

        search_paths = [self.data_repo_dir, "uploads", ".", "data"]
        found_candidates = []

        print(f"ğŸ” [ç³»ç»Ÿ] æ­£åœ¨æœç´¢å‚è€ƒæ–‡ä»¶ï¼Œå…³é”®è¯ï¼š[{target_pure}]")

        for search_dir in search_paths:
            if not os.path.exists(search_dir): continue
            for root, _, files in os.walk(search_dir):
                for f in files:
                    if f.startswith("~$") or f.startswith("."): continue
                    f_pure = f.lower().replace(" ", "")

                    # æ¨¡ç³ŠåŒ¹é…é€»è¾‘
                    if (target_pure in f_pure) or (os.path.splitext(f_pure)[0] in target_pure):
                        full_path = os.path.join(root, f)
                        found_candidates.append(full_path)
                        break
                if found_candidates: break

        if found_candidates:
            ref_path = found_candidates[0]

        if not ref_path:
            yield {"type": "text",
                   "data": f"âŒ åœ¨çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°å‚è€ƒæ–‡ä»¶ï¼š`{ref_filename}`ã€‚\n\n**ç³»ç»Ÿè§£æåˆ°çš„æ„å›¾ï¼š**\n- å‚è€ƒï¼š{ref_filename} (å»ç¡¬ç›˜æ‰¾è¿™ä¸ª)\n- æ–°å»ºï¼š{new_title}\n\nå»ºè®®ï¼šè¯·ç¡®è®¤ä¸Šä¼ çš„æ–‡ä»¶åæ˜¯å¦åŒ…å« `{target_pure}`ã€‚"}
            return

        yield {"type": "text", "data": f"âœ… å·²é”å®šå‚è€ƒæ–‡ä»¶ï¼š`{os.path.basename(ref_path)}`\n\n"}

        # =======================================================
        # 3. åç»­æµç¨‹ (ä½¿ç”¨ new_title ä½œä¸ºè¾“å‡ºæ–‡ä»¶å)
        # =======================================================
        try:
            # Load
            yield {"type": "status", "data": "ğŸ“– è§£æå‚è€ƒæ–‡æ¡£..."}
            self.writer_engine.load_reference(ref_path)

            # Style DNA
            yield {"type": "status", "data": "ğŸ§¬ æå–æ–‡é£ DNA..."}
            style_guide = self.writer_engine.analyze_style()
            yield {"type": "text", "data": f"> **æ–‡é£ DNA**ï¼š{style_guide}\n\n"}

            # Outline
            yield {"type": "status", "data": "ğŸ“‹ æ„æ€ç›®å½•..."}
            # å°†æ–°æ ‡é¢˜ä¹Ÿä¼ ç»™å¤§çº²ç”Ÿæˆå™¨ï¼Œä»¥ä¾¿ç”Ÿæˆæ›´å‡†ç¡®çš„æ ‡é¢˜
            full_project_info = f"é¡¹ç›®åç§°ï¼š{new_title}\nèƒŒæ™¯ä¿¡æ¯ï¼š{project_info}"
            new_toc = self.writer_engine.generate_outline(full_project_info)

            # 2.1 è¾“å‡ºç›®å½•ç»“æ„
            toc_preview = "\n".join([f"- {t}" for t in new_toc[:5]])
            yield {"type": "text", "data": f"**ç›®å½•æ¡†æ¶**ï¼š\n{toc_preview}\n... (å…±{len(new_toc)}ç« )\n\n"}
            yield {"type": "toc", "data": new_toc}

            # âœ… [UXä¼˜åŒ– 1] åœ¨è¿™é‡Œæ’å…¥â€œè€—æ—¶è¯´æ˜â€æç¤ºæ¡†
            yield {"type": "text", "data": """
---
#### â±ï¸ ç”Ÿæˆè€—æ—¶è¯´æ˜ï¼šå·²å¼€å¯â€œæ™ºèƒ½å®¡æ ¸â€æ¨¡å¼
ä¸ºäº†ç¡®ä¿æŠ¥å‘Šå†…å®¹çš„å‡†ç¡®æ€§ï¼Œç³»ç»Ÿæ­£åœ¨å¯¹æ¯ä¸€ä¸ªç« èŠ‚æ‰§è¡Œ **â€œåŒé‡æ ¡éªŒâ€ (ç”Ÿæˆ + æ·±åº¦å®¡æ ¸)**ï¼š
1. **ç”Ÿæˆ**ï¼šæ’°å†™åˆç¨¿ã€‚
2. **å®¡æ ¸**ï¼šæ£€æŸ¥å¹¶ä¿®æ­£å¯èƒ½å­˜åœ¨çš„æ—§åœ°åæ®‹ç•™æˆ–é€»è¾‘å¹»è§‰ã€‚

> âš ï¸ **æ³¨æ„**ï¼šæ­¤è¿‡ç¨‹ä¼šæ˜¾è‘—å¢åŠ ç”Ÿæˆæ—¶é—´ï¼ˆé¢„è®¡æ•´ä»½æŠ¥å‘Šéœ€ 3-5 åˆ†é’Ÿï¼‰ã€‚
> å¦‚æœæ‚¨åªéœ€è¦éƒ¨åˆ†å†…å®¹ï¼Œå»ºè®®åœ¨æé—®æ—¶æŒ‡å®šç« èŠ‚ï¼ˆä¾‹å¦‚ï¼šâ€œå¸®æˆ‘ç”Ÿæˆç¬¬ä¸‰ç« å»ºè®¾æ–¹æ¡ˆâ€ï¼‰ï¼Œé€Ÿåº¦ä¼šå¿«å¾ˆå¤šã€‚
---
"""}

            # Map
            yield {"type": "status", "data": "ğŸ”— å»ºç«‹æ˜ å°„..."}
            mapping = self.writer_engine.map_toc_relationships(new_toc)

            # Checkpoint Prep (ç¼“å­˜æ–‡ä»¶ç”¨ ref + new_title åš key)
            cache_key = hashlib.md5(f"{ref_filename}_{new_title}_{len(new_toc)}".encode()).hexdigest()
            cache_file = os.path.join("outputs", f"cache_{cache_key}.json")
            os.makedirs("outputs", exist_ok=True)

            generated_data = {}
            if os.path.exists(cache_file):
                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        generated_data = json.load(f)
                    yield {"type": "text",
                           "data": f"âš¡ **æ£€æµ‹åˆ°å†å²è¿›åº¦**ï¼Œå·²è‡ªåŠ¨æ¢å¤ {len(generated_data)} ä¸ªç« èŠ‚ã€‚\n\n"}
                except:
                    pass

            chapters_to_write = [t for t in new_toc if t not in generated_data]

            if not chapters_to_write:
                yield {"type": "text", "data": "ğŸ‰ æ‰€æœ‰ç« èŠ‚å‡å·²ç”Ÿæˆå®Œæ¯•ï¼Œç›´æ¥å¯¼å‡º...\n"}

            # å¹¶å‘æ‰§è¡Œ + æœ‰åºè¾“å‡º
            output_dir = "outputs"
            # âœ… [å…³é”®ä¿®æ”¹] ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ–°æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
            safe_title = new_title.replace("ã€Š", "").replace("ã€‹", "").replace(".docx", "")
            safe_title = re.sub(r'[\\/*?:"<>|]', "", safe_title)
            if not safe_title: safe_title = "æœªå‘½åæ ‡ä¹¦"

            output_filename = f"{safe_title}_{int(pd.Timestamp.now().timestamp())}.docx"
            output_path = os.path.join(output_dir, output_filename)

            total_tasks = len(chapters_to_write)
            if total_tasks > 0:
                MAX_WORKERS = 3
                yield {"type": "status", "data": f"ğŸš€ æ­£åœ¨å…¨é€Ÿæ’°å†™å¹¶å®¡æ ¸å‰©ä½™ {total_tasks} ç« ..."}

                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                    # æäº¤ä»»åŠ¡
                    future_to_title = {
                        executor.submit(
                            self.writer_engine.write_chapter,
                            title,
                            mapping.get(title),
                            full_project_info,
                            style_guide
                        ): title
                        for title in chapters_to_write
                    }

                    results_buffer = {}
                    next_idx_to_display = 0
                    completed_count = 0

                    for future in as_completed(future_to_title):
                        title = future_to_title[future]
                        try:
                            content = future.result()
                            results_buffer[title] = content
                            generated_data[title] = content
                            completed_count += 1

                            # âœ… [UXä¼˜åŒ– 2] çŠ¶æ€æ æ˜ç¡®æ˜¾ç¤ºâ€œæ­£åœ¨å®¡æ ¸â€
                            yield {"type": "status",
                                   "data": f"âœï¸ æ­£åœ¨æ’°å†™å¹¶å®¡æ ¸ï¼š({completed_count}/{total_tasks}) ä¸ªç« èŠ‚å®Œæˆ..."}

                            # ä¿å­˜ç¼“å­˜
                            with open(cache_file, 'w', encoding='utf-8') as f:
                                json.dump(generated_data, f, ensure_ascii=False)

                            # æœ‰åºæ˜¾ç¤º
                            while next_idx_to_display < len(chapters_to_write):
                                target_title = chapters_to_write[next_idx_to_display]
                                if target_title in results_buffer:
                                    preview = results_buffer[target_title][:50].replace("\n", "") + "..."
                                    yield {"type": "text", "data": f"âœ… **{target_title}**\n> {preview}\n\n"}
                                    next_idx_to_display += 1
                                else:
                                    break
                        except Exception as e:
                            print(f"ç”Ÿæˆé”™è¯¯: {e}")
                            yield {"type": "text", "data": f"âŒ **{title}** ç”Ÿæˆå¤±è´¥\n"}

            # Compile
            yield {"type": "status", "data": "ğŸ’¾ æ’ç‰ˆå¯¼å‡ºä¸­..."}
            ordered_content = {t: generated_data.get(t, "") for t in new_toc}
            self.writer_engine.compile_to_word(ordered_content, output_path)

            final_msg = f"""
### ğŸ‰ ç”Ÿæˆå®Œæˆï¼

**æ–‡ä»¶**ï¼š{output_filename}
**å‚è€ƒèŒƒæ–‡**ï¼š{os.path.basename(ref_path)}
**æ–‡é£**ï¼š{style_guide[:15]}...

è¯·ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ä¸‹è½½ã€‚
"""
            yield {"type": "text", "data": final_msg}
            yield {"type": "file", "data": {"path": output_path, "name": output_filename}}

        except Exception as e:
            traceback.print_exc()
            yield {"type": "text", "data": f"\nâŒ ä¸¥é‡é”™è¯¯: {str(e)}"}

    # ==========================================
    # ğŸš€ ä¸»æµç¨‹ (Standard RAG - å®Œæ•´é€»è¾‘)
    # ==========================================
    """
    === Pythonä»£ç æ–‡ä»¶: rag_service.py -> DeepSeekRAGService.chat_stream (V5.1 æœ€ç»ˆå®Œæ•´æ— çœç•¥ç‰ˆ) ===
    - åŒ…å«æ‚¨æä¾›çš„æ‰€æœ‰é€»è¾‘ï¼Œæœªç»ä»»ä½•çœç•¥ã€‚
    - æ³¨å…¥äº†å¯¹å¤æ‚ filter_config çš„æ”¯æŒï¼Œå¹¶ç¡®ä¿å®Œå…¨å‘åå…¼å®¹ã€‚
    """
    from typing import Union, List, Dict, Generator
    from langchain_core.documents import Document
    from langchain_core.output_parsers import StrOutputParser
    import traceback

    def chat_stream(self, query: str, history: List[Dict],
                    # å…¼å®¹æ—§ç‰ˆè°ƒç”¨ï¼Œä½†å»ºè®®åºŸå¼ƒ
                    top_k: int = 6,
                    project_filter: Union[str, List[str]] = None,
                    # ä¼˜å…ˆä½¿ç”¨æ–°çš„ã€ç»Ÿä¸€çš„é…ç½®å­—å…¸
                    filter_config: Dict = None) -> Generator[Dict, None, None]:

        # --- 0. é…ç½®èåˆä¸å‡†å¤‡ (æ–°å¢é€»è¾‘ï¼Œä¿è¯100%å…¼å®¹æ€§) ---
        if filter_config is None:
            # å¦‚æœè°ƒç”¨æ–¹æ²¡æœ‰ä¼ å…¥æ–°çš„ filter_configï¼Œåˆ™æ ¹æ®æ—§å‚æ•°ä¸´æ—¶æ„å»ºä¸€ä¸ª
            # è¿™ç¡®ä¿äº†å³ä½¿æ˜¯æ—§çš„å‰ç«¯æˆ–æµ‹è¯•è„šæœ¬ä¹Ÿèƒ½æ­£å¸¸å·¥ä½œ
            print("âš ï¸ [RAG Service] Warning: ä½¿ç”¨æ—§ç‰ˆå‚æ•°è°ƒç”¨ chat_streamï¼Œå»ºè®®åˆ‡æ¢åˆ° filter_configã€‚")
            filter_config = {
                "top_k": top_k,
                "project": project_filter or "æ‰€æœ‰é¡¹ç›®",
                "type": "æ‰€æœ‰ç±»å‹",
                "files": []
            }

        # ä»ç»Ÿä¸€çš„ filter_config ä¸­è·å–æ‰€æœ‰å‚æ•°ï¼Œä½œä¸ºåç»­æµç¨‹çš„å”¯ä¸€ä¿¡æº
        final_top_k = filter_config.get("top_k", 6)

        # --- 1. å†™ä½œæ„å›¾ä¾¦æµ‹ (å®Œæ•´ä¿ç•™) ---
        writing_keywords = ["æ’°å†™", "ç”Ÿæˆ", "å†™ä¸€ä»½", "ä»¿ç…§", "èµ·è‰", "ç¼–åˆ¶"]
        context_keywords = ["å‚è€ƒ", "æ ¹æ®", "åŸºäº", "æ¨¡ä»¿"]
        # å¦‚æœé—®é¢˜ä¸­åŒæ—¶åŒ…å«â€œæ’°å†™ç±»è¯æ±‡â€å’Œâ€œå‚è€ƒç±»è¯æ±‡â€ï¼Œåˆ™è½¬å…¥æ–‡ä¹¦ç”Ÿæˆ Agent
        if any(wk in query for wk in writing_keywords) and any(ck in query for ck in context_keywords):
            # å‡è®¾ self._run_writer_agent_stream æ˜¯æ‚¨å·²å®ç°çš„æ–¹æ³•
            if hasattr(self, '_run_writer_agent_stream'):
                for evt in self._run_writer_agent_stream(query):
                    yield evt
                return
            else:
                print("âš ï¸ [RAG Service] Warning: æ£€æµ‹åˆ°å†™ä½œæ„å›¾ï¼Œä½† _run_writer_agent_stream æ–¹æ³•æœªå®ç°ã€‚")

        # --- 2. é—®é¢˜é‡å†™ (å®Œæ•´ä¿ç•™) ---
        yield {"type": "status", "data": "ğŸ§  ä¼˜åŒ–æœç´¢é—®é¢˜..."}
        # å‡è®¾ self.rewrite_query æ˜¯æ‚¨å·²å®ç°çš„æ–¹æ³•
        optimized_query = self.rewrite_query(query) if hasattr(self, 'rewrite_query') else query
        yield {"type": "status", "data": f"ğŸ” æ£€ç´¢: {optimized_query}..."}

        # --- 3. æ··åˆæ£€ç´¢ ---
        fetch_k = final_top_k * 5

        # --- 3.1 å‘é‡æ£€ç´¢ Filter æ„å»º (æ ¸å¿ƒå¢å¼ºéƒ¨åˆ†) ---
        search_kwargs = {"k": fetch_k}

        conditions = []
        # æ¡ä»¶1: é¡¹ç›®èŒƒå›´
        project_scope = filter_config.get("project")
        if project_scope and project_scope != "æ‰€æœ‰é¡¹ç›®":
            if isinstance(project_scope, list) and len(project_scope) > 0:
                conditions.append({"project_name": {"$in": project_scope}})
            elif isinstance(project_scope, str):
                conditions.append({"project_name": {"$eq": project_scope}})

        # æ¡ä»¶2: æ–‡æ¡£ç±»å‹
        type_scope = filter_config.get("type")
        if type_scope and type_scope != "æ‰€æœ‰ç±»å‹":
            conditions.append({"category": {"$eq": type_scope}})

        # æ¡ä»¶3: å…·ä½“æ–‡ä»¶
        files_scope = filter_config.get("files")
        if files_scope and isinstance(files_scope, list) and len(files_scope) > 0:
            # å¦‚æœæœ‰æ–‡ä»¶çº§ç­›é€‰ï¼Œå®ƒçš„ä¼˜å…ˆçº§æœ€é«˜ï¼Œå¯ä»¥è¦†ç›–å…¶ä»–ç­›é€‰æ¡ä»¶ä»¥è·å¾—æœ€ç²¾ç¡®ç»“æœ
            conditions = [{"source_file": {"$in": files_scope}}]

        # ç»„åˆæ‰€æœ‰æ¡ä»¶
        if len(conditions) > 1:
            search_kwargs["filter"] = {"$and": conditions}
        elif len(conditions) == 1:
            search_kwargs["filter"] = conditions[0]

        print(f"âœ… [RAG Service] æ„å»ºçš„æœ€ç»ˆ filter: {search_kwargs.get('filter')}")

        # --- 3.2 æ‰§è¡Œå‘é‡æ£€ç´¢ (å®Œæ•´ä¿ç•™) ---
        vector_docs = []
        if self.vector_store:
            try:
                vector_docs = self.vector_store.as_retriever(search_type="similarity",
                                                             search_kwargs=search_kwargs).invoke(optimized_query)
            except Exception as e:
                print(f"å‘é‡æ£€ç´¢è­¦å‘Š: {e}")

        # --- 3.3 æ‰§è¡Œ BM25 æ£€ç´¢ (å®Œæ•´ä¿ç•™ï¼Œä»…é€‚é… filter_config) ---
        bm25_docs = []
        bm25_project_scope = filter_config.get("project")
        if self.bm25_manager and bm25_project_scope and bm25_project_scope != "æ‰€æœ‰é¡¹ç›®":
            try:
                target = bm25_project_scope if isinstance(bm25_project_scope, list) else [bm25_project_scope]
                raw = self.bm25_manager.search(query, target, top_k=5)
                for item in raw:
                    bm25_docs.append(Document(page_content=item['content'], metadata=item['metadata']))
            except Exception as e:
                print(f"BM25 æ£€ç´¢è­¦å‘Š: {e}")

        # --- 3.4 åˆå¹¶ä¸å»é‡ (å®Œæ•´ä¿ç•™) ---
        unique_ids = set()
        initial_docs = []
        # ä¼˜å…ˆä¿ç•™BM25çš„ç»“æœï¼Œå› ä¸ºå®ƒå¯¹äºå…³é”®è¯åŒ¹é…é€šå¸¸æ›´å‡†
        for d in bm25_docs + vector_docs:
            cid = d.metadata.get("chunk_id")
            if cid:
                if cid not in unique_ids:
                    d.metadata["source_method"] = "BM25" if d in bm25_docs else "Vector"
                    initial_docs.append(d)
                    unique_ids.add(cid)
            # å¯¹äºæ²¡æœ‰ chunk_id çš„æ–‡æ¡£ï¼Œå¯ä»¥æŒ‰å†…å®¹å»é‡ï¼ˆå…¼å®¹è€æ•°æ®ï¼‰
            elif d.page_content not in unique_ids:
                d.metadata["source_method"] = "BM25" if d in bm25_docs else "Vector"
                initial_docs.append(d)
                unique_ids.add(d.page_content)

        # --- 4. GraphRAG æ‰©å±• (å®Œæ•´ä¿ç•™) ---
        if initial_docs and self.graph_manager:
            try:
                yield {"type": "status", "data": "ğŸ•¸ï¸ æ‰©å±•å›¾è°±ä¸Šä¸‹æ–‡..."}
                expanded_docs = []
                # ä½¿ç”¨ `unique_ids` æ¥é¿å…é‡å¤æ·»åŠ å·²ç»å­˜åœ¨çš„å›¾è°±èŠ‚ç‚¹
                seen_graph_ids = unique_ids.copy()

                # å¯¹æ£€ç´¢ç»“æœä¸­å¾—åˆ†æœ€é«˜çš„å‰ 5 ä¸ªæ–‡æ¡£è¿›è¡Œå›¾è°±æ‰©å±•
                for d in initial_docs[:5]:
                    cid = d.metadata.get("chunk_id")
                    if cid:
                        # è·å– 1 è·³é‚»å±…
                        context_rows = self.graph_manager.get_context_window(cid, 1)
                        for row in context_rows:
                            for key in ['prev', 'next']:  # æ£€æŸ¥å‰åèŠ‚ç‚¹
                                node = row.get(key)
                                if node and node.get('id') not in seen_graph_ids:
                                    expanded_docs.append(Document(
                                        page_content=f"[å›¾è°±å…³è”] {node.get('text', '')}",
                                        metadata={"chunk_id": node.get('id'), "source_method": "Graph"}
                                    ))
                                    seen_graph_ids.add(node.get('id'))

                if expanded_docs:
                    print(f"ğŸ•¸ï¸ å›¾è°±æ‰©å±•äº† {len(expanded_docs)} ä¸ªæ–°ç‰‡æ®µ")
                    initial_docs.extend(expanded_docs)
            except Exception as e:
                print(f"âš ï¸ GraphRAG æ‰©å±•å¤±è´¥: {e}")

        # --- 5. Rerank (å®Œæ•´ä¿ç•™) ---
        if not initial_docs:
            context_str = "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚æˆ‘å°†åŸºäºé€šç”¨çŸ¥è¯†è¿›è¡Œå›ç­”ã€‚"
            final_docs = []
        else:
            if self.reranker:
                yield {"type": "status", "data": f"âš–ï¸ é‡æ’åº {len(initial_docs)} ä¸ªç‰‡æ®µ..."}
                final_docs = self.reranker.rerank(optimized_query, initial_docs, top_k=final_top_k)
            else:
                # å¦‚æœæ²¡æœ‰ rerankerï¼Œç›´æ¥æˆªå– top_k
                final_docs = initial_docs[:final_top_k]

            context_str = "\n\n".join([
                                          f"å¼•ç”¨ {i + 1} (æ¥æº: {d.metadata.get('source_file', 'æœªçŸ¥')}, æ–¹æ³•: {d.metadata.get('source_method', 'æœªçŸ¥')}):\n{d.page_content}"
                                          for i, d in enumerate(final_docs)])

        # --- 6. Pandas Agent (å®Œæ•´ä¿ç•™) ---
        excel_task_info = self._detect_excel_task(final_docs) if hasattr(self, '_detect_excel_task') else None
        if excel_task_info:
            file_path, sheet_name = excel_task_info
            yield {"type": "status", "data": "ğŸ“Š å¯åŠ¨ Pandas Agent åˆ†æExcel..."}
            try:
                # å‡è®¾ self._run_pandas_agent æ˜¯æ‚¨å·²å®ç°çš„æ–¹æ³•
                agent_result = self._run_pandas_agent(file_path, sheet_name, optimized_query)
                yield {"type": "text", "data": agent_result}
                # å³ä½¿AgentæˆåŠŸï¼Œä¹Ÿæä¾›ä¸€äº›åŸå§‹æ¥æºä½œä¸ºå‚è€ƒ
                yield {"type": "sources",
                       "data": [{"content": d.page_content, "metadata": d.metadata} for d in final_docs[:3]]}
                return
            except Exception as e:
                yield {"type": "text", "data": f"\nâš ï¸ Pandas Agent åˆ†æå¤±è´¥ï¼Œè½¬ä¸ºé€šç”¨å›ç­”æ¨¡å¼ã€‚é”™è¯¯: {e}\n\n"}

        # --- 7. æ ‡å‡† RAG ç”Ÿæˆ (å®Œæ•´ä¿ç•™) ---
        yield {"type": "sources", "data": [{"content": d.page_content, "metadata": d.metadata} for d in final_docs]}

        # æ„é€ å†å²å¯¹è¯è®°å½•
        chat_history_str = ""
        if history:
            for msg in history[-4:]:  # åªå–æœ€è¿‘4è½®å¯¹è¯ï¼Œé¿å…è¿‡é•¿
                chat_history_str += f"{msg['role']}: {msg['content']}\n"

        # å‡è®¾ self.prompt æ˜¯ä¸€ä¸ª PromptTemplate å¯¹è±¡
        chain = self.prompt | self.llm | StrOutputParser()
        try:
            # æµå¼è°ƒç”¨LLM
            for chunk in chain.stream({
                "chat_history": chat_history_str,
                "context": context_str,
                "question": optimized_query
            }):
                yield {"type": "text", "data": chunk}
        except Exception as e:
            traceback.print_exc()
            yield {"type": "text", "data": f"å¤§æ¨¡å‹è°ƒç”¨é”™è¯¯: {str(e)}"}

