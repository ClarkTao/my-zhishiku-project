"""
evaluation/run_eval.py
åŠŸèƒ½ï¼šåŸºäº RAGAS æ¡†æ¶è‡ªåŠ¨åŒ–è¯„ä¼° RAG ç³»ç»Ÿçš„å„é¡¹æŒ‡æ ‡ã€‚
æŒ‡æ ‡ï¼š
1. Faithfulness (å¿ å®åº¦): ç­”æ¡ˆæ˜¯å¦æœªç¼–é€ ï¼Ÿ
2. Answer Relevancy (ç­”æ¡ˆç›¸å…³æ€§): å›ç­”æ˜¯å¦åˆ‡é¢˜ï¼Ÿ
3. Context Precision (ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦): æ£€ç´¢åˆ°çš„å†…å®¹æ˜¯å¦æœ‰ç”¨ï¼Ÿ
"""

import os
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings

# è®¾ç½® Key
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

# 1. é…ç½® RAGAS ä½¿ç”¨ DeepSeek æ¨¡å‹ä½œä¸º"è£åˆ¤"
# RAGAS é»˜è®¤ç”¨ GPT-4 æ‰“åˆ†ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒæ¢æˆ DeepSeek
judge_llm = ChatOpenAI(
    model="deepseek-chat",
    openai_api_key=DEEPSEEK_API_KEY,
    openai_api_base="https://api.deepseek.com/v1",
    temperature=0
)

# 2. é…ç½® Embedding (ç”¨äºè®¡ç®—ç›¸å…³æ€§åˆ†æ•°)
# ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œé¿å…è°ƒç”¨ OpenAI Embedding äº§ç”Ÿè´¹ç”¨
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh-v1.5")

def create_test_data():
    """
    æ‰‹åŠ¨å®šä¹‰æˆ–ç”Ÿæˆæµ‹è¯•æ•°æ®é›†ã€‚
    æ ¼å¼è¦æ±‚ï¼š
    - question: é—®é¢˜
    - answer: RAGç”Ÿæˆçš„å›ç­”
    - contexts: æ£€ç´¢åˆ°çš„åŸæ–‡ç‰‡æ®µ (List[str])
    - ground_truth: æ ‡å‡†ç­”æ¡ˆ (äººå·¥æ’°å†™æˆ–ç”± GPT-4 ç”Ÿæˆ)
    """
    # è¿™é‡Œæ¨¡æ‹Ÿä¸€æ¬¡ RAG è¿è¡Œçš„ç»“æœ
    # åœ¨å®é™…å·¥ç¨‹ä¸­ï¼Œä½ åº”è¯¥å†™ä¸ªå¾ªç¯ï¼Œè·‘ä¸€é rag_serviceï¼ŒæŠŠç»“æœå­˜ä¸‹æ¥

    data_samples = {
        'question': ['åœŸæ–¹å¼€æŒ–çš„è¾¹å¡æ¯”ä¾‹æ˜¯å¤šå°‘ï¼Ÿ'],
        'answer': ['æ ¹æ®è§„èŒƒï¼ŒåœŸæ–¹å¼€æŒ–çš„è¾¹å¡æ¯”ä¾‹åº”æ§åˆ¶åœ¨ 1:0.5ã€‚'], # RAG ç”Ÿæˆçš„
        'contexts': [['åœŸæ–¹å¼€æŒ–é‡‡ç”¨è‡ªä¸Šè€Œä¸‹... å¼€æŒ–è¾¹å¡ä¸¥æ ¼æŒ‰ç…§ 1:0.5 æ§åˆ¶...']], # æ£€ç´¢åˆ°çš„
        'ground_truth': ['åœŸæ–¹å¼€æŒ–è¾¹å¡åº”ä¸º 1:0.5ã€‚'] # æ ‡å‡†ç­”æ¡ˆ
    }

    return Dataset.from_dict(data_samples)

def run_evaluation():
    print("ğŸš€ [Eval] å¼€å§‹ RAGAS è¯„ä¼°...")
    dataset = create_test_data()

    # è¿è¡Œè¯„ä¼°
    # ä¼ å…¥ judge_llm è®© DeepSeek å……å½“è£åˆ¤
    results = evaluate(
        dataset,
        metrics=[
            faithfulness,
            answer_relevancy,
            context_precision,
        ],
        llm=judge_llm,
        embeddings=embeddings
    )

    print("\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print(results)

    # å¯¼å‡ºä¸º Excel
    df = results.to_pandas()
    df.to_csv("rag_evaluation_report.csv", index=False)
    print("âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: rag_evaluation_report.csv")

if __name__ == "__main__":
    run_evaluation()
