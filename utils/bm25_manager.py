"""
utils/bm25_manager.py
åŠŸèƒ½ï¼šåŸºäºæ–‡ä»¶çš„ BM25 æŒä¹…åŒ–ç´¢å¼•ç®¡ç†å™¨
ç‰¹ç‚¹ï¼šæŒ‰é¡¹ç›®éš”ç¦»å­˜å‚¨ï¼Œæ”¯æŒå¢é‡æ›´æ–°ï¼Œæ”¯æŒå¤šé¡¹ç›®å¹¶å‘æ£€ç´¢
"""
import os
import pickle
import jieba
from rank_bm25 import BM25Okapi
from typing import List, Dict, Any


class BM25Persistence:
    def __init__(self, index_dir="bm25_indices"):
        self.index_dir = index_dir
        if not os.path.exists(index_dir):
            os.makedirs(index_dir)

    def _get_file_paths(self, project_name):
        # ä¸ºäº†æ–‡ä»¶ç³»ç»Ÿå®‰å…¨ï¼Œå¤„ç†ä¸€ä¸‹æ–‡ä»¶å
        safe_name = "".join([c for c in project_name if c.isalnum() or c in (' ', '_', '-')]).strip()
        base_path = os.path.join(self.index_dir, safe_name)
        return f"{base_path}_model.pkl", f"{base_path}_data.pkl"

    def _tokenize(self, text: str) -> List[str]:
        # ä½¿ç”¨ jieba æœç´¢å¼•æ“æ¨¡å¼åˆ†è¯
        return list(jieba.cut_for_search(text))

    def update_project_index(self, project_name: str, new_chunks: List[Dict]):
        """
        [å…¥åº“è°ƒç”¨] æ›´æ–°æŒ‡å®šé¡¹ç›®çš„ç´¢å¼•ï¼ˆæ”¯æŒå»é‡/è¦†ç›–æ›´æ–°ï¼‰
        """
        model_path, data_path = self._get_file_paths(project_name)

        # 1. åŠ è½½ç°æœ‰æ•°æ®
        existing_data = []
        if os.path.exists(data_path):
            try:
                with open(data_path, 'rb') as f:
                    existing_data = pickle.load(f)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ—§ç´¢å¼•æ•°æ®å¤±è´¥: {e}ï¼Œå°†é‡å»ºç´¢å¼•ã€‚")

        # âœ… [æ–°å¢] 2. ä¸¥è°¨çš„å»é‡åˆå¹¶é€»è¾‘ (Upsert Strategy)
        # ä½¿ç”¨å­—å…¸ key çš„å”¯ä¸€æ€§æ¥è‡ªåŠ¨å»é‡
        data_map = {}

        # 2.1 å…ˆè½½å…¥æ—§æ•°æ®
        for item in existing_data:
            # ä¼˜å…ˆä½¿ç”¨ chunk_id ä½œä¸ºä¸»é”®
            c_id = item.get('metadata', {}).get('chunk_id')
            if c_id:
                data_map[c_id] = item
            else:
                # å…¼å®¹æ€§å¤„ç†ï¼šä¸‡ä¸€æ—§æ•°æ®æ²¡ IDï¼Œç”¨å†…å®¹å“ˆå¸Œå…œåº• (æå°‘æƒ…å†µ)
                content_hash = hash(item.get('content', ''))
                data_map[f"hash_{content_hash}"] = item

        # 2.2 å†è½½å…¥æ–°æ•°æ® (å¦‚æœ ID ç›¸åŒï¼Œæ–°æ•°æ®ä¼šè¦†ç›–æ—§æ•°æ®)
        for item in new_chunks:
            c_id = item.get('metadata', {}).get('chunk_id')
            if c_id:
                data_map[c_id] = item  # è¦†ç›–ï¼
            else:
                # ç†è®ºä¸Š pipeline ä¿è¯äº†è‚¯å®šæœ‰ IDï¼Œè¿™é‡Œæ˜¯åŒä¿é™©
                content_hash = hash(item.get('content', ''))
                data_map[f"hash_{content_hash}"] = item

        # 2.3 è½¬å›åˆ—è¡¨
        full_data = list(data_map.values())

        print(
            f"ğŸ“Š æ•°æ®åˆå¹¶æŠ¥å‘Š: æ—§æ•°æ® {len(existing_data)} æ¡ + æ–°æ•°æ® {len(new_chunks)} æ¡ -> å»é‡åæ€»é‡ {len(full_data)} æ¡")

        if not full_data:
            return

        # 3. é‡æ–°æ„å»º BM25 ç´¢å¼• (ä¿æŒä¸å˜)
        print(f"ğŸ”„ æ­£åœ¨é‡å»ºé¡¹ç›® '{project_name}' çš„ BM25 ç´¢å¼•...")
        tokenized_corpus = [self._tokenize(doc['content']) for doc in full_data]
        bm25 = BM25Okapi(tokenized_corpus)

        # 4. æŒä¹…åŒ–ä¿å­˜ (ä¿æŒä¸å˜)
        with open(model_path, 'wb') as f:
            pickle.dump(bm25, f)
        with open(data_path, 'wb') as f:
            pickle.dump(full_data, f)

        print(f"âœ… BM25 ç´¢å¼•å·²æ›´æ–°å¹¶ä¿å­˜: {project_name}")

    def search(self, query: str, projects: List[str], top_k=3) -> List[Any]:
        """
        [æ£€ç´¢è°ƒç”¨] åœ¨æŒ‡å®šé¡¹ç›®åˆ—è¡¨ä¸­æœç´¢
        """
        results = []
        tokenized_query = self._tokenize(query)

        # éå†æ‰€æœ‰æ¶‰åŠçš„é¡¹ç›®
        # ä¼˜åŒ–ï¼šå¦‚æœæ˜¯â€œæ‰€æœ‰é¡¹ç›®â€ï¼Œè¿™é‡Œéœ€è¦éå†ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶ï¼ˆæš‚ç•¥ï¼Œå»ºè®®å‰ç«¯é™åˆ¶å¿…é¡»é€‰é¡¹ç›®ï¼‰
        target_projects = projects if isinstance(projects, list) else [projects]

        for proj in target_projects:
            if proj == "æ‰€æœ‰é¡¹ç›®": continue  # æš‚ä¸æ”¯æŒå…¨åº“ BM25ï¼Œå¤ªæ…¢

            model_path, data_path = self._get_file_paths(proj)
            if not os.path.exists(model_path):
                continue

            try:
                # åŠ è½½ç´¢å¼• (ä¼˜åŒ–ï¼šæœªæ¥å¯ä»¥ç”¨ LRU Cache ç¼“å­˜åˆ°å†…å­˜)
                with open(model_path, 'rb') as f:
                    bm25 = pickle.load(f)
                with open(data_path, 'rb') as f:
                    corpus_data = pickle.load(f)

                # è·å–åˆ†æ•°
                scores = bm25.get_scores(tokenized_query)
                # æ’åºå– Top-K
                top_n = bm25.get_top_n(tokenized_query, corpus_data, n=top_k)

                results.extend(top_n)

            except Exception as e:
                print(f"âš ï¸ æ£€ç´¢é¡¹ç›® {proj} å¤±è´¥: {e}")

        return results
